# German ELMo Model
This is a german [ELMo deep contextualized word representation](https://allennlp.org/elmo). It is trained on a special [German Wikipedia Text Corpus](https://github.com/t-systems-on-site-services-gmbh/german-wikipedia-text-corpus).

ELMo is a deep contextualized word representation that models both 
1. complex characteristics of word use (e.g., syntax and semantics)
2. how these uses vary across linguistic contexts (i.e., to model polysemy)

These word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. They can be easily added to existing models and significantly improve the state of the art across a broad range of challenging NLP problems, including question answering, textual entailment and sentiment analysis. More about these "Deep contextualized word representations" can be found here: https://arxiv.org/abs/1802.05365

This pretrained model can be used with [bilm-tf](https://github.com/allenai/bilm-tf) (the tensorflow implementation of ELMo). Before usage it can (should) be fine tuned with your domain specific data (see below). Unlike most other ELMo models we also provide the checkpoint files which enables you to do fine tuning.

## How this ELMo Model has been trained
This ELMo Model has been trained by using the [tensorflow version of bilm](https://github.com/allenai/bilm-tf). It was trained on the special [German Wikipedia Text Corpus](https://github.com/t-systems-on-site-services-gmbh/german-wikipedia-text-corpus).

The benefit of this text corpus is that it does not just contain the article space of the wiki. It also contains the comments for a larger text corpus and a more sloppy language. This should improve the quality of downstream tasks when you process conversations like mails, chats, tweets or support tickets.

The text corpus has been split to train and test set. The train set had 5.8GB of text data with 962,868,231 tokens. The test set had the size of 87MB.

The vocabulary file was generated by [this script](https://github.com/PhilipMay/de-wiki-text-corpus-tools/blob/master/vocab_file_writer.py). It limits the size of the vocabulary to the most frequently 800,000 tokens. This limitation is necessary to avoid out of memory errors during training time.

Now training was done like this:
1. Start training with [train_elmo.py](https://github.com/allenai/bilm-tf/blob/master/bin/train_elmo.py)
2. calculate perplexity on test set with [run_test.py](https://github.com/allenai/bilm-tf/blob/master/bin/run_test.py)
3. contunue training with [restart.py](https://github.com/allenai/bilm-tf/blob/master/bin/restart.py)
4. loop back to step 2 until the bilm starts to overfit

This has been done for 15 epochs. Epoch 15 was overfitting so the best perplexity was on epoch 14 with 40.170242 (see graphic below).

![Validation perplexity](https://raw.githubusercontent.com/t-systems-on-site-services-gmbh/german-elmo-model/master/perplexity-german-bilm.png "Validation perplexity")

Training time on a single GeForce GTX 1080 Ti was about 68 hours per epoch and test time like 2,5 hours. So this ELMo model was more then 44 days of computation time.

## How to use this ELMo Model
The usage of an ELMo model is described here:
- https://github.com/allenai/bilm-tf
- see these files for usage options:
  - [usage_cached.py](https://github.com/allenai/bilm-tf/blob/master/usage_cached.py)
  - [usage_character.py](https://github.com/allenai/bilm-tf/blob/master/usage_character.py)
  - [usage_token.py](https://github.com/allenai/bilm-tf/blob/master/usage_token.py)

For fine tuning see here: [How to do fine tune a model on additional unlabeled data?](https://github.com/allenai/bilm-tf#how-to-do-fine-tune-a-model-on-additional-unlabeled-data)

## Content of the Tarball
These files are needed to use the ELMo model:
- vocab-train.txt - vocabulary file
- weights.hdf5 - weights from the trained biLM in hdf5 format
- options.json - options file which describes the hyperparameters of the biLM

These files are part of the tensorflow checkpoint mechanism. It can save and restore TensorFlow models. This is needed to do fine tuning (see below):
- checkpoint
- model.ckpt-5265680.index
- model.ckpt-5265680.meta
- model.ckpt-5265680.data-00000-of-00001

## Download
- [bilm-output-result.tgz.part-00](https://github.com/t-systems-on-site-services-gmbh/german-elmo-model/releases/download/files_1/bilm-output-result.tgz.part-00)
  - MD5: 755379908885a6fa6fb5b68e4806eae0
  - SHA1: 39b16d8eaeaa6ff8c161d092fd223704cc3963e6
- [bilm-output-result.tgz.part-01](https://github.com/t-systems-on-site-services-gmbh/german-elmo-model/releases/download/files_1/bilm-output-result.tgz.part-01)
  - MD5: 0a6544017c64cda91a2b209ecbdc5f2c
  - SHA1: 4647c43037b01a67e6c206c9748c5e5b8ab936f8
- [bilm-output-result.tgz.part-02](https://github.com/t-systems-on-site-services-gmbh/german-elmo-model/releases/download/files_1/bilm-output-result.tgz.part-02)
  - MD5: f316774e8bd12d078bd5e869c64be14b
  - SHA1: 10530cb852cf26f6340c2b4d1cfbeb882f44692b
- [bilm-output-result.tgz.part-03](https://github.com/t-systems-on-site-services-gmbh/german-elmo-model/releases/download/files_1/bilm-output-result.tgz.part-03)
  - MD5: ae44a2d3cf95795edacba690b77a4e7b
  - SHA1: c54bcdc27d769e2c5a077d7ab342ee34dd1f3ec2
- bilm-output-result.tgz
  - MD5: 2aa41bbc32d42eae052a7ddf37909443
  - SHA1: 9af7efbdd480a919c45e11188625d59a5943f9b9

## Unpack
Using these commands you can unpack the files (Linux and macOS):
```
cat bilm-output-result.tgz.part-* > bilm-output-result.tgz
tar xvfz bilm-output-result.tgz
```
